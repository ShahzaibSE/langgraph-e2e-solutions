{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShahzaibSE/langgraph-e2e-solutions/blob/main/2_2_chains_reducers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5ZozuSDsEE6"
      },
      "source": [
        "# Chain\n",
        "\n",
        "## Review\n",
        "\n",
        "We built a simple graph with nodes, normal edges, and conditional edges.\n",
        "\n",
        "## Goals\n",
        "\n",
        "Now, let's build up to a simple chain that combines 4 [concepts](https://python.langchain.com/v0.2/docs/concepts/):\n",
        "\n",
        "* Using [chat messages](https://python.langchain.com/v0.2/docs/concepts/#messages) as our graph state\n",
        "* Using [chat models](https://python.langchain.com/v0.2/docs/concepts/#chat-models) in graph nodes\n",
        "* [Binding tools](https://python.langchain.com/v0.2/docs/concepts/#tools) to our chat model\n",
        "* [Executing tool calls](https://python.langchain.com/v0.2/docs/concepts/#functiontool-calling) in graph nodes\n",
        "\n",
        "![Screenshot 2024-08-21 at 9.24.03 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab08dd607b08df5e1101_chain1.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IOrz0hqFsFxQ"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langchain-google-genai langchain_core langgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "club_XV1snQ3"
      },
      "source": [
        "## Messages\n",
        "\n",
        "Chat models can use [`messages`](https://python.langchain.com/v0.2/docs/concepts/#messages), which capture different roles within a conversation.\n",
        "\n",
        "LangChain supports various message types, including `HumanMessage`, `AIMessage`, `SystemMessage`, and `ToolMessage`.\n",
        "\n",
        "These represent a message from the user, from chat model, for the chat model to instruct behavior, and from a tool call.\n",
        "\n",
        "Let's create a list of messages.\n",
        "\n",
        "Each message can be supplied with a few things:\n",
        "\n",
        "* `content` - content of the message\n",
        "* `name` - optionally, a message author\n",
        "* `response_metadata` - optionally, a dict of metadata (e.g., often populated by model provider for `AIMessages`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVIUmbVksoF-",
        "outputId": "0fe883c1-d362-4239-d83b-18c41b3c8259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: Model\n",
            "\n",
            "So you said you were researching ocean mammals?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "Name: Lance\n",
            "\n",
            "Yes, that's right.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: Model\n",
            "\n",
            "Great, what would you like to learn about.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "Name: Lance\n",
            "\n",
            "I want to learn about the best place to see Orcas in the US.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from pprint import pprint\n",
        "\n",
        "messages = [\n",
        "    AIMessage(content=f\"So you said you were researching ocean mammals?\", name=\"Model\")\n",
        "]\n",
        "\n",
        "messages.append(HumanMessage(content=f\"Yes, that's right.\",name=\"Lance\"))\n",
        "messages.append(AIMessage(content=f\"Great, what would you like to learn about.\", name=\"Model\"))\n",
        "messages.append(HumanMessage(content=f\"I want to learn about the best place to see Orcas in the US.\", name=\"Lance\"))\n",
        "\n",
        "for m in messages:\n",
        "  m.pretty_print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X62WM1kaXPWC"
      },
      "source": [
        "## Chat Models\n",
        "\n",
        "[Chat models](https://python.langchain.com/v0.2/docs/concepts/#chat-models) can use a sequence of message as input and support message types, as discussed above.\n",
        "\n",
        "There are [many](https://python.langchain.com/v0.2/docs/concepts/#chat-models) to choose from! Let's work with Gemini.\n",
        "\n",
        "Let's check that your `GEMINI_API_KEY` is set and, if not, you will be asked to enter it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WI7rS9ENXMtD"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "google_api_key = userdata.get('GEMINI_API_KEY')\n",
        "open_api_key = userdata.get('OPENAI_API_KEY')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJo9lx3FeF7g"
      },
      "source": [
        "We can load a chat model and invoke it with out list of messages.\n",
        "\n",
        "We can see that the result is an `AIMessage` with specific `response_metadata`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqrpQIoTeG6F"
      },
      "source": [
        "**Note:** Obtain the necessary json key from the Google Cloud Console by following the instructions outlined in step 21_langchain_ecosystem/langchain/-01_gemini_standalone/Gemini_API_python.ipynb file. Once acquired, load the json key in Google Colab to proceed with the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fcvrwpLHeBMX"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "gemini_llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", api_key=google_api_key)\n",
        "\n",
        "response = gemini_llm.invoke(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "AteggGzCp9LP",
        "outputId": "278a9c35-2d59-4213-ad57-717e75323861"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.messages.ai.AIMessage"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.messages.ai.AIMessage</b><br/>def __init__(content: Union[str, list[Union[str, dict]]], **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/langchain_core/messages/ai.py</a>Message from an AI.\n",
              "\n",
              "AIMessage is returned from a chat model as a response to a prompt.\n",
              "\n",
              "This message represents the output of the model and consists of both\n",
              "the raw output as returned by the model together standardized fields\n",
              "(e.g., tool calls, usage metadata) added by the LangChain framework.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 141);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "type(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XljnQe4Xgtwm",
        "outputId": "4832b98f-dabd-4567-c987-65630cb00939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "There's no single \"best\" place to see orcas in the US, as it depends on what kind of experience you're looking for and the time of year.  Orcas, or killer whales, have different populations with varying ranges.  However, some of the most popular and reliable locations include:\n",
            "\n",
            "* **Washington State (San Juan Islands):** This is arguably the most famous and consistently reliable place to see orcas in the US.  The resident orca population, known as the Southern Resident Killer Whales, frequents the waters around the San Juan Islands.  These orcas are easily identifiable and well-studied.  However, their population is critically endangered, so responsible whale watching is crucial.  Tours from Friday Harbor and other towns are readily available.\n",
            "\n",
            "* **Alaska:**  Alaska boasts several orca populations, including transients and residents.  Opportunities exist throughout the state, but popular areas include Southeast Alaska (Juneau, Ketchikan), Prince William Sound, and the Kenai Fjords.  Alaska offers a chance to see orcas in more wild and remote settings.  However, sightings are less guaranteed than in the San Juan Islands.\n",
            "\n",
            "* **California:** Orcas are seen less frequently in California than in Washington or Alaska, but they do pass through.  Opportunities exist along the coast, particularly in areas like Monterey Bay and the Channel Islands.  Sightings are less predictable.\n",
            "\n",
            "**To help me narrow it down, tell me:**\n",
            "\n",
            "* **What time of year are you planning to go?**  Orca sightings vary seasonally.\n",
            "* **What kind of experience are you looking for?**  A guided tour on a small boat? A larger whale watching vessel? A more remote experience?\n",
            "* **What's your budget?**  Tours and travel costs vary significantly between locations.\n",
            "\n",
            "Once I have this information, I can give you a more specific recommendation.\n"
          ]
        }
      ],
      "source": [
        "response.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy0jKXVVaZI5",
        "outputId": "1e4080b3-2cb2-4878-ec4a-fe87342e328a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='There\\'s no single \"best\" place to see orcas in the US, as it depends on what kind of experience you\\'re looking for and the time of year.  Orcas, or killer whales, have different populations with varying ranges.  However, some of the most popular and reliable locations include:\\n\\n* **Washington State (San Juan Islands):** This is arguably the most famous and consistently reliable place to see orcas in the US.  The resident orca population, known as the Southern Resident Killer Whales, frequents the waters around the San Juan Islands.  These orcas are easily identifiable and well-studied.  However, their population is critically endangered, so responsible whale watching is crucial.  Tours from Friday Harbor and other towns are readily available.\\n\\n* **Alaska:**  Alaska boasts several orca populations, including transients and residents.  Opportunities exist throughout the state, but popular areas include Southeast Alaska (Juneau, Ketchikan), Prince William Sound, and the Kenai Fjords.  Alaska offers a chance to see orcas in more wild and remote settings.  However, sightings are less guaranteed than in the San Juan Islands.\\n\\n* **California:** Orcas are seen less frequently in California than in Washington or Alaska, but they do pass through.  Opportunities exist along the coast, particularly in areas like Monterey Bay and the Channel Islands.  Sightings are less predictable.\\n\\n**To help me narrow it down, tell me:**\\n\\n* **What time of year are you planning to go?**  Orca sightings vary seasonally.\\n* **What kind of experience are you looking for?**  A guided tour on a small boat? A larger whale watching vessel? A more remote experience?\\n* **What\\'s your budget?**  Tours and travel costs vary significantly between locations.\\n\\nOnce I have this information, I can give you a more specific recommendation.\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-190eb929-4269-4944-902e-dfd116357c08-0', usage_metadata={'input_tokens': 46, 'output_tokens': 390, 'total_tokens': 436, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yizKB7YzZ0fM",
        "outputId": "9a880382-e554-4206-f329-a0a5cfe1eb39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},\n",
              " 'finish_reason': 'STOP',\n",
              " 'safety_ratings': []}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "response.response_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFqSPCg-6XOF"
      },
      "source": [
        "## Tools\n",
        "\n",
        "Tools are useful whenever you want a model to interact with external systems.\n",
        "\n",
        "External systems (e.g., APIs) often require a particular input schema or payload, rather than natural language.\n",
        "\n",
        "When we bind an API, for example, as a tool we given the model awareness of the required input schema.\n",
        "\n",
        "The model will choose to call a tool based upon the natural language input from the user.\n",
        "\n",
        "And, it will return an output that adheres to the tool's schema.\n",
        "\n",
        "[Many LLM providers support tool calling](https://python.langchain.com/v0.1/docs/integrations/chat/) and [tool calling interface](https://blog.langchain.dev/improving-core-tool-interfaces-and-docs-in-langchain/) in LangChain is simple.\n",
        "\n",
        "You can simply pass any Python `function` into `ChatModel.bind_tools(function)`.\n",
        "\n",
        "![Screenshot 2024-08-19 at 7.46.28 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab08dc1c17a7a57f9960_chain2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kKYG45Z96X8c"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XO1oNHLK8gjB"
      },
      "outputs": [],
      "source": [
        "def multiply(a:int, b:int):\n",
        "  \"\"\"Multiply a and b.\n",
        "\n",
        "  Args:\n",
        "      a: first int\n",
        "      b: second int\n",
        "  \"\"\"\n",
        "  return a * b\n",
        "\n",
        "gemini_llm_with_tools: ChatGoogleGenerativeAI = gemini_llm.bind_tools([multiply])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WqV-DFq02Op"
      },
      "source": [
        "**Note:** Warning indicates that the title key is being generated internally, likely within LangGraph or the Gemini integration when the tool is being bound, rather than being passed explicitly. So ignore the warning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXYbGJID02zv"
      },
      "source": [
        "If we pass an input - e.g., `\"What is 2 multiplied by 3\"` - we see a function_call returned.\n",
        "\n",
        "The function_call has specific arguments that match the input schema of our function along with the name of the function to call.\n",
        "\n",
        "```\n",
        "{'name': 'multiply', 'arguments': '{\"b\": 3.0, \"a\": 2.0}'}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "345YGPmN2PBa"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vAGjY772YED",
        "outputId": "a8ac94e0-0d03-4b1c-d735-a8962cc5f530"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"a\": 3.0, \"b\": 9.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-598cd0f4-3008-4f2a-9a23-979b46ce0c10-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3.0, 'b': 9.0}, 'id': '09575676-27f6-4cec-9dda-962dc79a7c87', 'type': 'tool_call'}], usage_metadata={'input_tokens': 62, 'output_tokens': 3, 'total_tokens': 65, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "function_call = gemini_llm_with_tools.invoke([HumanMessage(content=\"What will be the result of multiplication 3 and 9?\", name='Shazy')])\n",
        "function_call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLSIIk0oC_Zd",
        "outputId": "a36431e1-81fb-4390-b7fc-242ea7ccb957"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'multiply', 'arguments': '{\"a\": 3.0, \"b\": 9.0}'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "function_call.additional_kwargs['function_call']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddnVuoxh0rdt",
        "outputId": "fd0eb8f1-f347-461a-e3ee-917cdf9d1f79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"a\": 2.0, \"b\": 3.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-a954acb7-5699-4a76-9235-bce664eccfc3-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2.0, 'b': 3.0}, 'id': '33f574c6-a343-49e9-b6d4-062777ef7e8a', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 3, 'total_tokens': 60, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# function_call = gemini_llm_with_tools.invoke(HumanMessage(content=\"What will be the result of multiplication 3 and 2?\", name='Shazy'))\n",
        "function_call = gemini_llm_with_tools.invoke([HumanMessage(content=f\"What is 2 multiplied by 3\", name=\"Lance\")])\n",
        "function_call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXShlbnEC50t",
        "outputId": "3479c791-6a04-4242-95ca-51e47903cca9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'multiply', 'arguments': '{\"a\": 2.0, \"b\": 3.0}'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "function_call.additional_kwargs['function_call']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN2PbSn3j0jM"
      },
      "source": [
        "## Using messages as state\n",
        "\n",
        "With these foundations in place, we can now use [`messages`](https://python.langchain.com/v0.2/docs/concepts/#messages) in our graph state.\n",
        "\n",
        "Let's define our state, `MessagesState`, as a `TypedDict` with a single key: `messages`.\n",
        "\n",
        "`messages` is simply a list of messages, as we defined above (e.g., `HumanMessage`, etc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AIunRt8wj1c2"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "from langchain_core.messages import AnyMessage\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "  messages:list[AnyMessage]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJO01n5Tmkck"
      },
      "source": [
        "## Reducers\n",
        "\n",
        "Now, we have a minor problem!\n",
        "\n",
        "As we discussed, each node will return a new value for our state key `messages`.\n",
        "\n",
        "But, this new value will [will override](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) the prior `messages` value.\n",
        "\n",
        "As our graph runs, we want to **append** messages to to our `messages` state key.\n",
        "\n",
        "We can use [reducer functions](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) address this.\n",
        "\n",
        "Reducers allow us to specify how state updates are performed.\n",
        "\n",
        "If no reducer function is specified, then it is assumed that updates to the key should *override it* as we saw before.\n",
        "\n",
        "But, to append messages, we can use the pre-built `add_messages` reducer.\n",
        "\n",
        "This ensures that any messages are appended to the existing list of messages.\n",
        "\n",
        "We annotate simply need to annotate our `messages` key with the `add_messages` reducer function as metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rr09MiCemjKx"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class MessageState(TypedDict):\n",
        "  messages:Annotated[list[AnyMessage], add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds7VJrz_sPRv"
      },
      "source": [
        "Since having a list of messages in graph state is so common, LangGraph has a pre-built [`MessagesState`](https://langchain-ai.github.io/langgraph/concepts/low_level/#messagesstate)!\n",
        "\n",
        "`MessagesState` is defined:\n",
        "\n",
        "* With a pre-build single `messages` key\n",
        "* This is a list of `AnyMessage` objects\n",
        "* It uses the `add_messages` reducer\n",
        "\n",
        "We'll usually use `MessagesState` because it is less verbose than defining a custom `TypedDict`, as shown above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "InuTKu04sGSD"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import MessagesState\n",
        "\n",
        "class MessagesState(MessagesState):\n",
        "    # Add any keys needed beyond messages, which is pre-built\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial state\n",
        "initial_messages = [AIMessage(content=\"Hello! How can I assist you?\", name=\"Model\"),\n",
        "                    HumanMessage(content=\"I'm looking for information on marine biology.\", name=\"Lance\")\n",
        "                   ]\n",
        "\n",
        "# New message to add\n",
        "new_message = AIMessage(content=\"Sure, I can help with that. What specifically are you interested in?\", name=\"Model\")\n",
        "\n",
        "# Calling Reducer.\n",
        "add_messages(initial_messages, new_message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k7xrpv0AVhI",
        "outputId": "07fbfef3-a467-41c2-ff0a-9ed939493ed5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='Hello! How can I assist you?', additional_kwargs={}, response_metadata={}, name='Model', id='72bddb95-bc61-4315-9d17-4c9939eaf802'),\n",
              " HumanMessage(content=\"I'm looking for information on marine biology.\", additional_kwargs={}, response_metadata={}, name='Lance', id='359d9c75-f736-4069-84e1-048301780908'),\n",
              " AIMessage(content='Sure, I can help with that. What specifically are you interested in?', additional_kwargs={}, response_metadata={}, name='Model', id='e9165d36-9ed3-440c-855b-7c7722d7b012')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langchain_google_genai langchain_core langgraph"
      ],
      "metadata": {
        "id": "Y9ItDYXvBHNO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating Graph**"
      ],
      "metadata": {
        "id": "G1_arQrVAuow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        "def tool_calling_llm(state:MessagesState):\n",
        "  return {'messages':gemini_llm_with_tools.invoke(state['messages'])}\n",
        "\n",
        "# Build graph\n",
        "builder: StateGraph = StateGraph(MessagesState)\n",
        "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
        "# Adding edges.\n",
        "builder.add_edge(START, \"tool_calling_llm\")\n",
        "builder.add_edge(\"tool_calling_llm\", END)\n",
        "\n",
        "graph: CompiledStateGraph = builder.compile()\n",
        "\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "SyqqtgDFA49g",
        "outputId": "2858fe0f-f82a-4aa8-cb37-6c6ca252c196"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = graph.invoke({'messages': HumanMessage(content=\"Hello!\")})\n",
        "\n",
        "for message in messages['messages']:\n",
        "  message.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMFHVyQtS3nZ",
        "outputId": "c95232e6-3a21-457b-8445-79c645d709e4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hello!\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = graph.invoke({\"messages\": HumanMessage(content=\"Multiply 4 and 2!\")})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ-VI6T6UIYM",
        "outputId": "6fcb3598-2463-409d-99fd-8b2ac2c8a47a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply 4 and 2!\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (b24f3cad-57e7-4086-8318-cf9c43f43d84)\n",
            " Call ID: b24f3cad-57e7-4086-8318-cf9c43f43d84\n",
            "  Args:\n",
            "    a: 4.0\n",
            "    b: 2.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSCBAtjwFSDq9RuBFJq3R7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}